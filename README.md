# Neural-Net-from-Scratch
### NN from Scratch - No Tensorflow

A simple neural network with 1 hidden layer trained on MNIST dataset without using deep learning frameworks such as Tensorflow, Pytorch, etc. This network achieves 90% accuracy on train set and 91% on test set.

My goal was to deeply understand the underlyting math in neural nets and how they really make it work. I implemented a few activation functions, namely reLU, leaky reLU,
softmax, sigmoid, tanh and their derivatives using only numpy.

I also used matplotlib to demonstrate a few samples from the test set.
